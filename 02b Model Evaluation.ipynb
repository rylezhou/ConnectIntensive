{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Connect Intensive - Machine Learning Nanodegree\n",
    "### Lesson 02: Building and evaluating models with `sklearn`\n",
    "\n",
    "## Objectives\n",
    "  - Use the `pandas` library to learn how to preprocess data \n",
    "  - Use the `sklearn` library to build a predictive `DecisionTreeClassifier` model for the Titanic Survival Dataset.\n",
    "  - Compute the accuracy of a model on both the training and validation (testing) data.\n",
    "  - Adjust hyperparameters (e.g. `max_depth`) to see the effects on model accuracy.\n",
    "  - (Optional) Visualize the model to understand how complexity leads to high variance.\n",
    "  \n",
    "#### Optional Prerequisites\n",
    "  - Optional prerequisites for visualizing the decision trees.\n",
    "    - Install [graphviz](http://graphviz.org/), graph visualization software.\n",
    "      - If you use [homebrew](http://brew.sh/), there's [a brew formula available for graphviz](http://brewformulas.org/Graphviz)\n",
    "    - Install [pydotplus](https://pypi.python.org/pypi/pydotplus), a Python interface to Graphviz's Dot language\n",
    "\n",
    "\n",
    "## Acknowledgements\n",
    "  - This lesson is adapted from part 2 of Thomas Corcoran's excellent [`sklearn` tutorial](https://github.com/tccorcoran/Connect/tree/master/sklearn-tutorial). Thank you Thomas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "As usual, we start by importing some useful libraries and modules. Don't worry if you get a warning message when importing `matplotlib` -- it just needs to build the font cache, and the warning is just to alert you that this may take a while the first time the cell is run.\n",
    "\n",
    "**Run** the cell below (**click** the cell to highlight it, then press **shift + enter** or **shift + return**) to import useful libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "try:\n",
    "    #import matplotlib\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    print(\"Successfully imported matplotlib.pyplot! (Version {})\")\n",
    "except ImportError:\n",
    "    print(\"Could not import matplotlib.pyplot!\")\n",
    "    \n",
    "try:\n",
    "    import numpy as np\n",
    "    print(\"Successfully imported numpy! (Version {})\".format(np.version.version))\n",
    "except ImportError:\n",
    "    print(\"Could not import numpy!\")\n",
    "    \n",
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Successfully imported pandas! (Version {})\".format(pd.__version__))\n",
    "    pd.options.display.max_rows = 10\n",
    "except ImportError:\n",
    "    print(\"Could not import pandas!\")\n",
    "\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    print(\"Successfully imported display from IPython.display!\")\n",
    "except ImportError:\n",
    "    print(\"Could not import display from IPython.display\")\n",
    "    \n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"Successfully imported sklearn! (Version {})\".format(sklearn.__version__))\n",
    "    skversion = int(sklearn.__version__[2:4])\n",
    "except ImportError:\n",
    "    print(\"Could not import sklearn!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data from the Titanic project\n",
    "\n",
    "#### *You will need to specify the proper path to the dataset file or copy the file into the same directory as this notebook*\n",
    "\n",
    "\n",
    "**Run** the cell below (**click** on the cell to highlight it, then press **shift + enter** or **shift + return** to run it) to read the preprocessed training and testing data into `pandas` `DataFrame` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./titanic_data.csv\")\n",
    "\n",
    "print(\"Titanic data sets loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All machine learning projects start with data exploration. In this phase, you look to see what the data looks like, check if there are values missing and prepare the data for training with `sklearn`. Preparing the data includes converting data to the right types (numeric only), scaling so the actual values of the features are of \"reasonable\" and similar size.\n",
    "\n",
    "Here we will practice handling missing values and converting categorical data into numeric.\n",
    "\n",
    "First, though, lets do a cursory examination of the data using `describe`, `head` or `info`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO - Use the three methods listed above to get a general sense of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the output of `decribe`, you will that the `count` for `Age` is lower than the count for all the other columns. This indicates that there are missing values in `Age` and we will need to do some preprocessing. This step is called `imputation`. One way to get rid of mising values is to replace them with a value derived from the data, e.g., the avergae. This is what we will do. Run the cell below to replace all missing `Age` values with the average over all the rows with values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.loc[train_df.Age.isnull(),'Age'] = train_df.Age.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables can be replaced by a numerical map (an encoding). The `get_dummies` method is useful for this. In our dataset, we have `Sex` and `Embarked` (port where the passenger got on the ship). Run the cell below to apply a method known as one-hot-encoding and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.get_dummies(train_df, columns=[\"Sex\", \"Embarked\"], prefix=[\"sex\", \"port\"])\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to use the preprocessed data to build a predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further dividing the data\n",
    "To train and evaluate our model, we split the available data into a training set and a test set. The test set should be used *only for evaluating* the expected performance on unseen (future) data. If we want to tune hyper-parameters of a model, compare the performance of different algorithms for model *selection*, we need to further split our training set into two parts -- the actual training set and a validation set. \n",
    "\n",
    "  - **Training set:** A set of examples used for machine learning, that is to fit the parameters (*i.e.*, weights) of the classifier.\n",
    "  - **Validation set:** A set of examples used to tune the hyperparameters (*i.e.*, architecture, not weights) of a classifier, for example to choose the maximum depth of a decision tree, or the number of hidden layers in a neural network.\n",
    "  - **Test set:** A set of examples used only *once*. This assesses the performance (generalization) of the fully-specified classifier (once all hyperparameters have been specified).\n",
    "\n",
    "So you can think of the three groups of data this way: you build a collection of models from the same training data set. The models might all be slightly different (*e.g.* decision trees of maximum depths of 1, 2, 3, 4, ...). You then apply each of these models to the validation data set. Based on how well each of these models performs, you may select an optimal depth, which was the adjustable *hyperparameter* for the decision tree classifier. Finally, once the model and all of its adjustable parameters have been decided upon, this model can be applied to the testing data to see how well it generalizes to unknown data.\n",
    "\n",
    "For more on testing vs. validation data sets, you can consult the [test set wikipedia article](https://en.wikipedia.org/wiki/Test_set) or [this Quora post on training and testing data](https://www.quora.com/What-is-a-training-data-set-test-data-set-in-machine-learning-What-are-the-rules-for-selecting-them).\n",
    "\n",
    "In this case, we are working with a toy model and learn about what could go wrong, so we are going to use the test set as our validation set. In practive or in projects where you care about getting good results, you should use three sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `train_test_split`\n",
    "The latest version of the library `sklearn` has the module `model_selection`, which contains [the method `train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). We can use this method to split `train_df` further into a training and a validation set. The arguments that we need to pass to `train_test_split()` are:\n",
    "  - `X` & `y`: Arrays. These can be `pandas` `DataFrame` or `Series` objects.\n",
    "  - `test_size`: A float with the proportion of data to put into the test set. *e.g.* `test_size = 0.1` would put one in every ten instances into the test set.\n",
    "  - `random_state`: The pseudo-random number generator state used for random sampling. For a given value of `random_state`, the method will partition the data set exactly the same way each time, which is useful for debugging.\n",
    "\n",
    "**Run** the cell below to create the `DataFrame` object `X` and the `Series` object `y` from the desired features from `train_df`. Then use `train_test_split()` with a `random_state` to split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with scikit-learn version 0.18, the model_selection module replaces the cross_validation module,\n",
    "# so we should import train_test_split from the appropriate module depending on the version number.\n",
    "if skversion >= 18:\n",
    "    from sklearn.model_selection import train_test_split\n",
    "else:\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# A list of the desired feature names for model building\n",
    "# We are skipping Passenger ID as that is too specific and is more a row label than a feature\n",
    "# Same arguments lead us to exclude `Cabin` and `Ticket`\n",
    "desired_features = ['Pclass', 'sex_female', 'sex_male', 'Age','SibSp','Parch', 'Fare', 'port_C', 'port_S', 'port_Q']\n",
    "\n",
    "# X is our pandas DataFrame object with the features from which we will predict the 'Survived' feature.\n",
    "X = pd.DataFrame(train_df[desired_features])\n",
    "\n",
    "# y is our pandas Series object with the 'Survived' feature to be predicted.\n",
    "y = pd.Series(train_df['Survived'])\n",
    "\n",
    "# Split the data into training and validation (test) data sets:\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "# Take a look at the first few rows of the training features and classes\n",
    "display(X_train.head())\n",
    "display(y_train.head())\n",
    "\n",
    "# Verify that the data sets were split 80% training and 20% testing\n",
    "print(\"The original data ({} instances) was split into training ({} instances) and testing ({} instances) data sets\".\\\n",
    "     format(len(X),len(X_train),len(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the indices on `X_train.head()` and `y_train.head()` -- they match, as expected, but they're shuffled, which may be unexpected! Also, the print statement above shows us that `train_test_split()` is partitioning the data based on the given `test_size` argument. Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree Classifier\n",
    "For supervised learning problems, the model building `sklearn` workflow is pretty similar, regardless of the type of classifier you'd like to build:\n",
    "  1. **Create** a classifier object.\n",
    "  2. **Train** the classifier on the training data set.\n",
    "  3. **Predict** with the classifier on the validation (test) data set.\n",
    "  4. **Assess** the accuracy of the classifier, comparing the predictions to the actual labels.\n",
    "\n",
    "Let's try it here! Let's build a [Decision Tree Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) of `max_depth` 1. We will need to import `DecisionTreeClassifier` and `accuracy_score` from the appropriate `sklearn` modules.\n",
    "\n",
    "**Run** the cell below to **create** a Decision Tree Classifier, **train** it on the training data, **predict** class labels for the validation (test) data set, and **assess** the accuracy of the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeClassifier, f1-score and accuracy_score from the appropriate sklearn modules\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#TODO - Steps 1 - 4\n",
    "# 1. CREATE the classifier object... in this example, call it clf1. Use the defaults for all the hyperparameters\n",
    "clf1=DecisionTreeClassifier()\n",
    "\n",
    "# 2. TRAIN the classifier object o X_train, y_train using the method .fit()\n",
    "clf1.fit(X_train, y_train)\n",
    "\n",
    "# 3. PREDICT labels for the validation (test) set using the method .predict()\n",
    "y_pred_train = 0\n",
    "y_pred_test  = 0\n",
    "\n",
    "# 4. ASSESS the accuracy of the classifier, comparing the predictions to the actual labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the accuracy on the training data is great, but our accuracy on the testing data is not.\n",
    "\n",
    "As part of the fit process, the classifier records the relative importance of each of the features in an attribute called `feature_importances_`. They are listed in column order. Run the next cell to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = zip(clf1.feature_importances_, X_train.columns)\n",
    "features.sort(reverse=True)\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing a Decision Tree (Optional)\n",
    "Some advantages to a decision tree classifier are that the classification process is simple to understand and interpret, and the decision tree can be easily visualized. Looking into the `sklearn` [documentation on Decision Trees](http://scikit-learn.org/stable/modules/tree.html), we see that there is a method, `export_graphviz`, where we can visualize the decision tree in [Graphviz format](http://www.graphviz.org/). Getting this working on your machine may be a little tricky, because there's an additional application and Python module needed, but I think the payoff is worth it.\n",
    "  1. You need to install the [Graphviz](http://www.graphviz.org/) application. Note that [`pip install graphviz`](https://pypi.python.org/pypi/graphviz) is **not** sufficient. It does not install the Graphviz application.\n",
    "  2. You need to install [the module `pydotplus`](https://pypi.python.org/pypi/pydotplus)\n",
    "  \n",
    "Once you've completed the installation prerequisites above, **run** the cell below to visualize the Decision Tree Classifier you created above with `max_depth` 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "dot_data = export_graphviz(clf1, out_file=None, \n",
    "                         feature_names=desired_features,  \n",
    "                         class_names=['Died','Survived'],  \n",
    "                         filled=True, rounded=True,  \n",
    "                         special_characters=True)  \n",
    "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
